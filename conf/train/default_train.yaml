# optimization
gradient_acc_steps: 8
gradient_clip_value: 10.0
max_steps: 1200000

# evaluation and persistence
apply_early_stopping: True
val_check_interval: 0.5
val_percent_check: 0.1
monitor_var: "val_loss"
monitor_var_mode: "min"
patience: 50

# core
gpus: 1
precision: 16
amp_level:

checkpoint_path:

learning_rate: 0.00005
weight_decay: 0.0
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 0.00000001

seed: 42

lr_scheduler: "inverse_square_root"
warmup_steps: 1000